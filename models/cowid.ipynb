{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560b8408-eead-4f95-9c52-4fba9ceef653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 171 images belonging to 33 classes.\n",
      "Found 30 images belonging to 33 classes.\n",
      "Class labels: {'COW': 0, 'COW (10)': 1, 'COW (11)': 2, 'COW (12)': 3, 'COW (13)': 4, 'COW (14)': 5, 'COW (15)': 6, 'COW (16)': 7, 'COW (17)': 8, 'COW (18)': 9, 'COW (19)': 10, 'COW (2)': 11, 'COW (20)': 12, 'COW (21)': 13, 'COW (22)': 14, 'COW (23)': 15, 'COW (24)': 16, 'COW (25)': 17, 'COW (26)': 18, 'COW (27)': 19, 'COW (28)': 20, 'COW (29)': 21, 'COW (3)': 22, 'COW (30)': 23, 'COW (31)': 24, 'COW (32)': 25, 'COW (33)': 26, 'COW (4)': 27, 'COW (5)': 28, 'COW (6)': 29, 'COW (7)': 30, 'COW (8)': 31, 'COW (9)': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sagar Panchal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\Sagar Panchal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 634ms/step - accuracy: 0.0506 - loss: 3.7526 - val_accuracy: 0.0667 - val_loss: 3.4771\n",
      "Epoch 2/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 504ms/step - accuracy: 0.0398 - loss: 3.4898 - val_accuracy: 0.0667 - val_loss: 3.4473\n",
      "Epoch 3/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 548ms/step - accuracy: 0.0822 - loss: 3.4937 - val_accuracy: 0.0667 - val_loss: 3.4554\n",
      "Epoch 4/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 522ms/step - accuracy: 0.0394 - loss: 3.4666 - val_accuracy: 0.0667 - val_loss: 3.3876\n",
      "Epoch 5/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 507ms/step - accuracy: 0.1051 - loss: 3.3359 - val_accuracy: 0.1333 - val_loss: 3.3521\n",
      "Epoch 6/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 504ms/step - accuracy: 0.0655 - loss: 3.3107 - val_accuracy: 0.1333 - val_loss: 3.2459\n",
      "Epoch 7/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 513ms/step - accuracy: 0.0826 - loss: 3.2503 - val_accuracy: 0.1333 - val_loss: 3.2162\n",
      "Epoch 8/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 510ms/step - accuracy: 0.0909 - loss: 3.1429 - val_accuracy: 0.1667 - val_loss: 2.9772\n",
      "Epoch 9/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 509ms/step - accuracy: 0.1352 - loss: 3.0613 - val_accuracy: 0.2333 - val_loss: 2.8884\n",
      "Epoch 10/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 517ms/step - accuracy: 0.1736 - loss: 2.8785 - val_accuracy: 0.3333 - val_loss: 2.7230\n",
      "Epoch 11/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 513ms/step - accuracy: 0.2117 - loss: 2.6771 - val_accuracy: 0.3333 - val_loss: 2.6542\n",
      "Epoch 12/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 515ms/step - accuracy: 0.1929 - loss: 2.6835 - val_accuracy: 0.3000 - val_loss: 2.4413\n",
      "Epoch 13/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 519ms/step - accuracy: 0.2785 - loss: 2.4079 - val_accuracy: 0.2667 - val_loss: 2.2553\n",
      "Epoch 14/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 515ms/step - accuracy: 0.2448 - loss: 2.2743 - val_accuracy: 0.4000 - val_loss: 2.1385\n",
      "Epoch 15/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 506ms/step - accuracy: 0.3292 - loss: 2.1912 - val_accuracy: 0.4000 - val_loss: 2.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as cow_identification_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001ABE44BDA80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001ABE44BDA80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "📷 Image: C:\\Users\\Sagar Panchal\\OneDrive\\Desktop\\Cows\\COW (11)\\WhatsApp Image 2025-06-05 at 17.36.52_457502cd.jpg\n",
      "🐄 Predicted Cow ID: COW (7)\n",
      "🎯 Confidence: 53.20%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Set dataset paths\n",
    "base_dir = r\"C:\\Users\\Sagar Panchal\\OneDrive\\Desktop\\Cows\"  \n",
    "\n",
    "# Image properties\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data preprocessing with augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,  # 80% train, 20% validation\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Number of cow classes\n",
    "num_classes = len(train_generator.class_indices)\n",
    "print(\"Class labels:\", train_generator.class_indices)\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Output: one node per cow\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"cow_id.h5\")\n",
    "print(\"✅ Model saved as cow_identification_model.h5\")\n",
    "\n",
    "# Reload the saved model for prediction\n",
    "model = tf.keras.models.load_model(\"cow_id.h5\")\n",
    "\n",
    "# Get class labels in correct order\n",
    "class_indices = train_generator.class_indices\n",
    "# Reverse mapping from index to label\n",
    "class_labels = [None] * len(class_indices)\n",
    "for label, index in class_indices.items():\n",
    "    class_labels[index] = label\n",
    "\n",
    "# Prediction function\n",
    "def predict_cow_id(img_path):\n",
    "    # Load and preprocess image\n",
    "    img = image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    predicted_label = class_labels[predicted_index]\n",
    "    confidence = np.max(prediction) * 100\n",
    "\n",
    "    print(f\"📷 Image: {img_path}\")\n",
    "    print(f\"🐄 Predicted Cow ID: {predicted_label}\")\n",
    "    print(f\"🎯 Confidence: {confidence:.2f}%\")\n",
    "\n",
    "# === Test with your image ===\n",
    "test_image_path = r\"C:\\Users\\Sagar Panchal\\OneDrive\\Desktop\\Cows\\COW (11)\\WhatsApp Image 2025-06-05 at 17.36.52_457502cd.jpg\"\n",
    "predict_cow_id(test_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ad34d7-004e-46e0-b47c-9ed6a58bb5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\SAGARP~1\\AppData\\Local\\Temp\\tmpc9ddqibx\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\SAGARP~1\\AppData\\Local\\Temp\\tmpc9ddqibx\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\SAGARP~1\\AppData\\Local\\Temp\\tmpc9ddqibx'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name='input_layer_10')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 33), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1837774663952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1837770851536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1837778834768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1837778834384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1837778834576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1837778832080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1837778829584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1837778832464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1837778831888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1837778835152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "✅ TFLite model saved as cow_eye.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the Keras model\n",
    "model = tf.keras.models.load_model(\"cow_id.h5\")\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# (Optional) Enable optimizations for smaller model size (recommended for mobile)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the .tflite model\n",
    "with open(\"cow_id.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"✅ TFLite model saved as cow_eye.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e447e15f-ff51-4311-b484-d728f235a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: C:\\Users\\Sagar Panchal\\OneDrive\\Desktop\\Cows\n",
      "Found 171 images belonging to 33 classes.\n",
      "Found 30 images belonging to 33 classes.\n",
      "Class labels (index mapping): {'COW': 0, 'COW (10)': 1, 'COW (11)': 2, 'COW (12)': 3, 'COW (13)': 4, 'COW (14)': 5, 'COW (15)': 6, 'COW (16)': 7, 'COW (17)': 8, 'COW (18)': 9, 'COW (19)': 10, 'COW (2)': 11, 'COW (20)': 12, 'COW (21)': 13, 'COW (22)': 14, 'COW (23)': 15, 'COW (24)': 16, 'COW (25)': 17, 'COW (26)': 18, 'COW (27)': 19, 'COW (28)': 20, 'COW (29)': 21, 'COW (3)': 22, 'COW (30)': 23, 'COW (31)': 24, 'COW (32)': 25, 'COW (33)': 26, 'COW (4)': 27, 'COW (5)': 28, 'COW (6)': 29, 'COW (7)': 30, 'COW (8)': 31, 'COW (9)': 32}\n",
      "✅ Class indices saved to models\\cow_class_indices.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sagar Panchal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,735,104</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,257</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m4,735,104\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)                  │           \u001b[38;5;34m4,257\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,832,609</span> (18.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,832,609\u001b[0m (18.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,832,609</span> (18.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,832,609\u001b[0m (18.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training for 15 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sagar Panchal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932ms/step - accuracy: 0.0107 - loss: 3.7105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sagar Panchal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.0116 - loss: 3.7020 - val_accuracy: 0.1000 - val_loss: 3.4696\n",
      "Epoch 2/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 883ms/step - accuracy: 0.0305 - loss: 3.4726 - val_accuracy: 0.0667 - val_loss: 3.3949\n",
      "Epoch 3/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 885ms/step - accuracy: 0.0688 - loss: 3.4164 - val_accuracy: 0.1000 - val_loss: 3.2810\n",
      "Epoch 4/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 915ms/step - accuracy: 0.0978 - loss: 3.2831 - val_accuracy: 0.1000 - val_loss: 3.2046\n",
      "Epoch 5/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 896ms/step - accuracy: 0.1423 - loss: 3.2452 - val_accuracy: 0.1000 - val_loss: 3.0133\n",
      "Epoch 6/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 860ms/step - accuracy: 0.1700 - loss: 3.0997 - val_accuracy: 0.2000 - val_loss: 2.8677\n",
      "Epoch 7/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 863ms/step - accuracy: 0.1644 - loss: 2.8933 - val_accuracy: 0.2667 - val_loss: 2.6929\n",
      "Epoch 8/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 875ms/step - accuracy: 0.1791 - loss: 2.8385 - val_accuracy: 0.4333 - val_loss: 2.4345\n",
      "Epoch 9/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 882ms/step - accuracy: 0.2616 - loss: 2.6264 - val_accuracy: 0.4667 - val_loss: 2.5348\n",
      "Epoch 10/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 874ms/step - accuracy: 0.2333 - loss: 2.6800 - val_accuracy: 0.4000 - val_loss: 2.3844\n",
      "Epoch 11/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 892ms/step - accuracy: 0.2622 - loss: 2.6190 - val_accuracy: 0.3667 - val_loss: 2.2755\n",
      "Epoch 12/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 848ms/step - accuracy: 0.3431 - loss: 2.2449 - val_accuracy: 0.4667 - val_loss: 1.9071\n",
      "Epoch 13/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 906ms/step - accuracy: 0.4382 - loss: 1.9385 - val_accuracy: 0.5667 - val_loss: 1.7590\n",
      "Epoch 14/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 897ms/step - accuracy: 0.4201 - loss: 1.8784 - val_accuracy: 0.6333 - val_loss: 1.7130\n",
      "Epoch 15/15\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 900ms/step - accuracy: 0.4513 - loss: 1.9390 - val_accuracy: 0.5333 - val_loss: 1.5809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as models\\cow_id.h5\n",
      "\n",
      "Training complete. You can now use 'cow_id.h5' and 'cow_class_indices.json' in your web app.\n",
      "\n",
      "--- Testing reloaded model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step\n",
      "📷 Test Image: C:\\Users\\Sagar Panchal\\OneDrive\\Desktop\\Cows\\COW (11)\\WhatsApp Image 2025-06-05 at 17.36.52_457502cd.jpg\n",
      "🐄 Predicted Cow ID: COW (11)\n",
      "🎯 Confidence: 49.58%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import json # Import json to save class indices\n",
    "\n",
    "# Set dataset paths\n",
    "# IMPORTANT: Adjust this path to where your cow images are located\n",
    "base_dir = r\"C:\\Users\\Sagar Panchal\\OneDrive\\Desktop\\Cows\"\n",
    "\n",
    "# Image properties\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15 # Define epochs as a variable\n",
    "\n",
    "# Data preprocessing with augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,  # 80% train, 20% validation\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest' # Added fill_mode for augmented images\n",
    ")\n",
    "\n",
    "print(f\"Loading images from: {base_dir}\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Number of cow classes\n",
    "num_classes = len(train_generator.class_indices)\n",
    "print(\"Class labels (index mapping):\", train_generator.class_indices)\n",
    "\n",
    "# --- SAVE CLASS INDICES ---\n",
    "# This is crucial for your web app to correctly map predictions back to cow names\n",
    "model_dir = \"models\" # Define a directory for models and related files\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "with open(os.path.join(model_dir, \"cow_class_indices.json\"), \"w\") as f:\n",
    "    json.dump(train_generator.class_indices, f)\n",
    "print(f\"✅ Class indices saved to {os.path.join(model_dir, 'cow_class_indices.json')}\")\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Output: one node per cow\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary() # Print model summary\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nStarting model training for {EPOCHS} epochs...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(os.path.join(model_dir, \"cow_id.h5\"))\n",
    "print(f\"✅ Model saved as {os.path.join(model_dir, 'cow_id.h5')}\")\n",
    "\n",
    "print(\"\\nTraining complete. You can now use 'cow_id.h5' and 'cow_class_indices.json' in your web app.\")\n",
    "\n",
    "# --- Optional: Test the reloaded model for a single prediction ---\n",
    "# You can uncomment and modify this section if you want to test prediction\n",
    "# with a single image directly from this script after training.\n",
    "\n",
    "print(\"\\n--- Testing reloaded model ---\")\n",
    "# Reload the saved model for prediction\n",
    "model_reloaded = tf.keras.models.load_model(os.path.join(model_dir, \"cow_id.h5\"))\n",
    "\n",
    "# Get class labels in correct order\n",
    "# You need to load them back just like in the app.py\n",
    "loaded_class_indices = {}\n",
    "with open(os.path.join(model_dir, \"cow_class_indices.json\"), \"r\") as f:\n",
    "    loaded_class_indices = json.load(f)\n",
    "\n",
    "class_labels_reloaded = [None] * len(loaded_class_indices)\n",
    "for label, index in loaded_class_indices.items():\n",
    "    class_labels_reloaded[index] = label\n",
    "\n",
    "# Prediction function for testing\n",
    "def predict_cow_id_test(img_path_test):\n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path_test, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0) # Add batch dimension\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model_reloaded.predict(img_array)\n",
    "        predicted_index = np.argmax(prediction[0]) # Get index from the first (and only) sample in the batch\n",
    "        predicted_label = class_labels_reloaded[predicted_index]\n",
    "        confidence = np.max(prediction[0]) * 100\n",
    "\n",
    "        print(f\"📷 Test Image: {img_path_test}\")\n",
    "        print(f\"🐄 Predicted Cow ID: {predicted_label}\")\n",
    "        print(f\"🎯 Confidence: {confidence:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during test prediction for {img_path_test}: {e}\")\n",
    "\n",
    "# === Test with your image ===\n",
    "# IMPORTANT: Change this path to a real image in your dataset for testing\n",
    "test_image_path = r\"C:\\Users\\Sagar Panchal\\OneDrive\\Desktop\\Cows\\COW (11)\\WhatsApp Image 2025-06-05 at 17.36.52_457502cd.jpg\"\n",
    "predict_cow_id_test(test_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc55fc12-2b22-4773-80ab-078dd0d49cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
